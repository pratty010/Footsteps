{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods:\n",
    "- **name**: The unique name of the agent.\n",
    "- **description**: The description of the agent in text.\n",
    "- **on_messages()**: Send the agent a sequence of ChatMessage get a Response. It is important to note that agents are expected to be stateful and this method is expected to be called with new messages, not the complete history.\n",
    "- **on_messages_stream()**: Same as on_messages() but returns an iterator of AgentEvent or ChatMessage followed by a Response as the last item.\n",
    "- **on_reset()**: Reset the agent to its initial state.\n",
    "\n",
    "See autogen_agentchat.messages for more information on AgentChat message types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Assistant Agent](https://microsoft.github.io/autogen/0.4.0.dev13/user-guide/agentchat-user-guide/tutorial/agents.html#assistant-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AssistantAgent is a built-in agent that uses a language model and has the ability to use tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the required and basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a basic tool to return static answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool that searches the web for information.\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Find information on the web\"\"\"\n",
    "    return \"AutoGen is a programming framework for building multi-agent applications.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To create an instance of a client to local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent that uses the local llm\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"ollama/llama3.2:3b\",\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    "    base_url=\"http://0.0.0.0:4000/\",\n",
    "    model_capabilities={\n",
    "        \"json_output\": False,\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": True,\n",
    "        },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an Assistant Agent with capabilities to call the `web_search` func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[web_search],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get response using the `on_message` function which gives the final answer from the agentic worflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to the on_messages() method returns a Response that contains the agent’s final response in the chat_message attribute, as well as a list of inner messages in the inner_messages attribute, which stores the agent’s “thought process” that led to the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assistant_run() -> None:\n",
    "    response = await agent.on_messages(\n",
    "        [TextMessage(content=\"Find information on AutoGen\", source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    "    print(f\"Internal messages >\\n {response.inner_messages}\")\n",
    "    print(f\"Final answer >\\n {response.chat_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal messages >\n",
      " [ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=251, completion_tokens=17), content=[FunctionCall(id='call_703b50f0-0017-4420-b052-2923e5e8fdc2', arguments='{\"query\": \"AutoGen\"}', name='web_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', call_id='call_703b50f0-0017-4420-b052-2923e5e8fdc2')], type='ToolCallExecutionEvent')]\n",
      "Final answer >\n",
      " source='assistant' models_usage=None content='AutoGen is a programming framework for building multi-agent applications.' type='ToolCallSummaryMessage'\n"
     ]
    }
   ],
   "source": [
    "# Use asyncio.run(assistant_run()) when running in a script.\n",
    "await assistant_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get response using the `on_messages_stream` function which stream each message as it is generated by the agentic worflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assistant_run_stream() -> None:\n",
    "    # Option 1: read each message from the stream (as shown in the previous example).\n",
    "    # async for message in agent.on_messages_stream(\n",
    "    #     [TextMessage(content=\"Find information on AutoGen\", source=\"user\")],\n",
    "    #     cancellation_token=CancellationToken(),\n",
    "    # ):\n",
    "    #     print(message)\n",
    "\n",
    "    # Option 2: use Console to print all messages as they appear.\n",
    "    await Console(\n",
    "        agent.on_messages_stream(\n",
    "            [TextMessage(content=\"Find information on AutoGen\", source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- assistant ----------\n",
      "[FunctionCall(id='call_65c140a6-a194-40f9-9843-4030be71b36e', arguments='{\"query\": \"AutoGen\"}', name='web_search')]\n",
      "[Prompt tokens: 354, Completion tokens: 18]\n",
      "---------- assistant ----------\n",
      "[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', call_id='call_65c140a6-a194-40f9-9843-4030be71b36e')]\n",
      "---------- assistant ----------\n",
      "AutoGen is a programming framework for building multi-agent applications.\n",
      "---------- Summary ----------\n",
      "Number of inner messages: 2\n",
      "Total prompt tokens: 354\n",
      "Total completion tokens: 18\n",
      "Duration: 0.34 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use asyncio.run(assistant_run_stream()) when running in a script.\n",
    "await assistant_run_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tools](https://microsoft.github.io/autogen/0.4.0.dev13/user-guide/agentchat-user-guide/tutorial/agents.html#using-tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this limitation, modern LLMs can now accept a list of available tool schemas (descriptions of tools and their arguments) and generate a tool call message. This capability is known as `Tool Calling or Function Calling` and is becoming a popular pattern in building intelligent agent-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AgentChat, the AssistantAgent can use tools to perform specific actions. The web_search tool is one such tool that allows the assistant agent to search the web for information. A custom tool can be a Python function or a subclass of the `BaseTool`.\n",
    "\n",
    "By default, when AssistantAgent executes a tool, it will return the tool’s output as a string in `ToolCallSummaryMessage` in its response. If your tool does not return a well-formed string in natural language, you can add a reflection step to have the model summarize the tool’s output, by setting the `reflect_on_tool_use=True` parameter in the AssistantAgent constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\n",
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "from autogen_ext.tools.code_execution import PythonCodeExecutionTool\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a docker executor for the python excution tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tool.\n",
    "code_executor = DockerCommandLineCodeExecutor(\n",
    "    work_dir=\"/home/ace/Everything/PlayGround/GenAI-Learnings/AutoGen_Learnings/data/code_exec/\",\n",
    "    )\n",
    "# code_executor = LocalCommandLineCodeExecutor()\n",
    "await code_executor.start()\n",
    "code_execution_tool = PythonCodeExecutionTool(code_executor)\n",
    "cancellation_token = CancellationToken()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoking the tool directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the tool directly without an agent.\n",
    "code = \"print('Hello, world!')\"\n",
    "result = await code_execution_tool.run_json({\"code\": code}, cancellation_token)\n",
    "print(code_execution_tool.return_value_as_string(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using an assistant agent to answer the questions based on the limited history and avaiable tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, AssistantAgent uses the UnboundedChatCompletionContext which sends the full conversation history to the model. To limit the context to the last n messages, you can use the BufferedChatCompletionContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AssistantAgent(\n",
    "    name = \"assistant\",\n",
    "    tools=[code_execution_tool],\n",
    "    model_client=model_client,\n",
    "    system_message=\"Dataset is available to use as variable `df`. Use tools to solve tasks. Install the necessary dependencies as required.\",\n",
    "    model_context=BufferedChatCompletionContext(buffer_size=10)\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"/home/ace/Everything/PlayGround/GenAI-Learnings/AutoGen_Learnings/data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await Console(\n",
    "    agent.on_messages_stream(\n",
    "        [TextMessage(content=\"What's the average age of the passengers in pandas dataframe df as defined above?\", source=\"user\")], CancellationToken()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Proxy Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UserProxyAgent is a built-in agent that provides one way for a user to intervene in the process. This agent will put the team in a temporary blocking state, and thus any exceptions or runtime failures while in the blocked state will result in a deadlock. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import UserProxyAgent\n",
    "\n",
    "async def user_proxy_run() -> None:\n",
    "    user_proxy_agent = UserProxyAgent(\n",
    "        name=\"user_proxy\",\n",
    "    )\n",
    "    response = await user_proxy_agent.on_messages(\n",
    "        [TextMessage(content=\"What is your name? \", source=\"user\")], cancellation_token=CancellationToken()\n",
    "    )\n",
    "    print(f\"Your name is {response.chat_message.content}\")\n",
    "\n",
    "\n",
    "# Use asyncio.run(user_proxy_run()) when running in a script.\n",
    "await user_proxy_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
